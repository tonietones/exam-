{
  "version": 4,
  "terraform_version": "1.4.2",
  "serial": 4,
  "lineage": "49dadeb2-676c-20b7-5009-c71525035edf",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "aws_eks_cluster",
      "name": "demo",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "arn": "arn:aws:eks:us-east-1:249042606135:cluster/demo",
            "certificate_authority": [
              {
                "data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUMvakNDQWVhZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJek1ETXhOekV4TURJeU9Wb1hEVE16TURNeE5ERXhNREl5T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTW9xCkxqWndPSDcrbnMvVzFJUXluL0xWVXdkUzdkMndrZUZzY0kwelhGL2lybUYzVmJMUThZV05Bb3c0RzdTMmRLaE0KdlZiTDdnY2N2S0FlWm9XM1MyMmFjTjc0UU9TMy80VGhWRXRZQ0VMc3NhWmVCeG45bUgwRUFiNWptd05zT1BhSwpvczhBYVJBVzVqWCt4UFF6d3Z1cTR1RURSOHlPU2JDeWc0SEdQNFpWeGg0SlBlakJIL2dqSC9qSW90Zm9tSUJRCmhnZi84S3lDREhKOTJJSWw4bkgxSWZ4OUhza0ZtckY2dnZHdjBUNm1tQTRzMCs2T1dPT1VmMjFwdzhpU2NKNDAKcVJzVHArNkFkalRmdHplTlkyQUE0M3A1STkvb1JQWXUwdDd5alBxNGlpZFMrNmRqTnhnTFhzaHRLZTJQd3VsQgp0OEZQSXJ5TU1tTkNnd0YraEcwQ0F3RUFBYU5aTUZjd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZGS3o3L0I0QzA3SSs2K0FBY0tIMnkvUVJBSHZNQlVHQTFVZEVRUU8KTUF5Q0NtdDFZbVZ5Ym1WMFpYTXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBS0tGVmZBVFFWcExFQU9jQzh5UgpZNHlPMkFocFc4VlByeWlhTERMRmRrYVdNS0lWVVpiV2RHYnF3aEEyMnZ1K29vVWVIMkp3S1R5RStsaFpDRnBSCkxvVGVFeVdtbmwyMzV1OXJ2S3JHczFWYk9YamcrT3h1Sy9ZRFJNQTJSZHdML0puL3JzZlFFb3ZoaSsyTTAydE0KTXpJQlY1Wk0xMDhxOFQwSE1OYXF1akgrUlhDdHQwS3RiYk5CdmJncGlpaHRndVh5ZWZlYnc3N1ZUcGpKaDIrWQptTG15R0hzTDlid0V0MXozZWJ0NjRGb2h2RmkrM01iZ3JMaHAxa1JSM0h5Sk5mbDhwaGFlVXNOQzZ3bnNMa3FaCnlLR0MvTGs2UTdZWWw3Nm90eDUrNHI5bEc1Ly9qUE9xaW1PUWlUNXF0YnR3YmVoTDRUVnF6Q3NtY3pPYnNsancKVEp3PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg=="
              }
            ],
            "cluster_id": null,
            "created_at": "2023-03-17 10:55:56.465 +0000 UTC",
            "enabled_cluster_log_types": [],
            "endpoint": "https://2C67DB241227BCC60469936B2788D027.gr7.us-east-1.eks.amazonaws.com",
            "id": "demo",
            "identity": [
              {
                "oidc": [
                  {
                    "issuer": "https://oidc.eks.us-east-1.amazonaws.com/id/2C67DB241227BCC60469936B2788D027"
                  }
                ]
              }
            ],
            "kubernetes_network_config": [
              {
                "ip_family": "ipv4",
                "service_ipv4_cidr": "172.20.0.0/16",
                "service_ipv6_cidr": ""
              }
            ],
            "name": "demo",
            "outpost_config": [],
            "platform_version": "eks.1",
            "role_arn": "arn:aws:iam::249042606135:role/eks-cluster-demo",
            "status": "ACTIVE",
            "tags": {},
            "version": "1.25",
            "vpc_config": [
              {
                "cluster_security_group_id": "sg-087e8f1974e7506da",
                "endpoint_private_access": false,
                "endpoint_public_access": true,
                "public_access_cidrs": [
                  "0.0.0.0/0"
                ],
                "security_group_ids": [],
                "subnet_ids": [
                  "subnet-01e706db0d87a7146",
                  "subnet-036c36b62816049a1",
                  "subnet-09f03da0502fe7b7d",
                  "subnet-0dc4f0ffc9fadc325"
                ],
                "vpc_id": "vpc-01dc850ff6d9b5d00"
              }
            ]
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_cluster_auth",
      "name": "demo_auth",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "demo_auth",
            "name": "demo_auth",
            "token": "k8s-aws-v1.aHR0cHM6Ly9zdHMuYW1hem9uYXdzLmNvbS8_QWN0aW9uPUdldENhbGxlcklkZW50aXR5JlZlcnNpb249MjAxMS0wNi0xNSZYLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFUVDdBWVRBM1hGU0dYTlNEJTJGMjAyMzAzMTglMkZ1cy1lYXN0LTElMkZzdHMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDIzMDMxOFQyMTU2NTNaJlgtQW16LUV4cGlyZXM9MCZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QlM0J4LWs4cy1hd3MtaWQmWC1BbXotU2lnbmF0dXJlPWIzN2M4ZTY5MzllNGM2OTk1NDY1ODU2MjA2ZDZjMjJlYTM1MzdkNzg3ZGJkYTllYTMyZDZiYTY4OWIyY2E4MGI"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "data",
      "type": "aws_eks_node_group",
      "name": "eks-node-group",
      "provider": "provider[\"registry.terraform.io/hashicorp/aws\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "ami_type": "AL2_x86_64",
            "arn": "arn:aws:eks:us-east-1:249042606135:nodegroup/demo/private-nodes/c6c37b36-8736-f42a-6a8c-7e2022308040",
            "capacity_type": "ON_DEMAND",
            "cluster_name": "demo",
            "disk_size": 20,
            "id": "demo:private-nodes",
            "instance_types": [
              "t2.xlarge"
            ],
            "labels": {
              "role": "general"
            },
            "node_group_name": "private-nodes",
            "node_role_arn": "arn:aws:iam::249042606135:role/eks-node-group-nodes",
            "release_version": "1.25.6-20230304",
            "remote_access": [],
            "resources": [
              {
                "autoscaling_groups": [
                  {
                    "name": "eks-private-nodes-c6c37b36-8736-f42a-6a8c-7e2022308040"
                  }
                ],
                "remote_access_security_group_id": ""
              }
            ],
            "scaling_config": [
              {
                "desired_size": 3,
                "max_size": 5,
                "min_size": 0
              }
            ],
            "status": "ACTIVE",
            "subnet_ids": [
              "subnet-036c36b62816049a1",
              "subnet-0dc4f0ffc9fadc325"
            ],
            "tags": {},
            "taints": [],
            "version": "1.25"
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.63.0",
                "chart": "kube-prometheus-stack",
                "name": "prometheus",
                "namespace": "prometheus",
                "revision": 1,
                "values": "{\"alertRelabelConfigs\":{},\"alertmanager\":{\"enabled\":true,\"persistence\":{\"size\":\"2Gi\"},\"podSecurityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534}},\"configmapReload\":{\"prometheus\":{\"containerSecurityContext\":{},\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraVolumeDirs\":[],\"image\":{\"digest\":\"\",\"pullPolicy\":\"IfNotPresent\",\"repository\":\"jimmidyson/configmap-reload\",\"tag\":\"v0.8.0\"},\"name\":\"configmap-reload\",\"resources\":{}}},\"extraManifests\":[],\"extraScrapeConfigs\":\"\",\"forceNamespace\":\"\",\"imagePullSecrets\":[],\"kube-state-metrics\":{\"enabled\":true},\"networkPolicy\":{\"enabled\":false},\"podSecurityPolicy\":{\"enabled\":true},\"prometheus-node-exporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false},\"enabled\":true,\"rbac\":{\"pspEnabled\":false}},\"prometheus-pushgateway\":{\"enabled\":true,\"serviceAnnotations\":{\"prometheus.io/probe\":\"pushgateway\"}},\"rbac\":{\"create\":true},\"ruleFiles\":{},\"server\":{\"affinity\":{},\"alertmanagers\":[],\"baseURL\":\"\",\"configMapOverrideName\":\"\",\"configPath\":\"/etc/config/prometheus.yml\",\"containerSecurityContext\":{},\"defaultFlagsOverride\":[],\"deploymentAnnotations\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"enableServiceLinks\":true,\"env\":[],\"extraArgs\":{},\"extraConfigmapLabels\":{},\"extraConfigmapMounts\":[],\"extraFlags\":[\"web.enable-lifecycle\"],\"extraHostPathMounts\":[],\"extraInitContainers\":[],\"extraSecretMounts\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"global\":{\"evaluation_interval\":\"1m\",\"scrape_interval\":\"1m\",\"scrape_timeout\":\"10s\"},\"hostAliases\":[],\"hostNetwork\":false,\"image\":{\"digest\":\"\",\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/prometheus\",\"tag\":\"\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraLabels\":{},\"extraPaths\":[],\"hosts\":[],\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"livenessProbeFailureThreshold\":3,\"livenessProbeInitialDelay\":30,\"livenessProbePeriodSeconds\":15,\"livenessProbeSuccessThreshold\":1,\"livenessProbeTimeout\":10,\"name\":\"server\",\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":false,\"existingClaim\":\"\",\"labels\":{},\"mountPath\":\"/data\",\"size\":\"8Gi\",\"subPath\":\"\"},\"podAnnotations\":{},\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":1},\"podLabels\":{},\"podSecurityPolicy\":{\"annotations\":{}},\"prefixURL\":\"\",\"priorityClassName\":\"\",\"probeHeaders\":[],\"probeScheme\":\"HTTP\",\"readinessProbeFailureThreshold\":3,\"readinessProbeInitialDelay\":30,\"readinessProbePeriodSeconds\":5,\"readinessProbeSuccessThreshold\":1,\"readinessProbeTimeout\":4,\"remoteRead\":[],\"remoteWrite\":[],\"replicaCount\":1,\"resources\":{},\"retention\":\"15d\",\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"annotations\":{},\"clusterIP\":\"\",\"enabled\":true,\"externalIPs\":[],\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":80,\"sessionAffinity\":\"None\",\"statefulsetReplica\":{\"enabled\":false,\"replica\":0},\"type\":\"ClusterIP\"},\"sidecarContainers\":{},\"sidecarTemplateValues\":{},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":30,\"periodSeconds\":5,\"timeoutSeconds\":10},\"statefulSet\":{\"annotations\":{},\"enabled\":false,\"headless\":{\"annotations\":{},\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"servicePort\":80},\"labels\":{},\"podManagementPolicy\":\"OrderedReady\"},\"storagePath\":\"\",\"strategy\":{\"type\":\"Recreate\"},\"tcpSocketProbeEnabled\":false,\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"verticalAutoscaler\":{\"enabled\":false}},\"server.resources\":\"\\\"limits\\\":\\n  \\\"cpu\\\": \\\"200m\\\"\\n  \\\"memory\\\": \\\"50Mi\\\"\\n\\\"requests\\\":\\n  \\\"cpu\\\": \\\"100m\\\"\\n  \\\"memory\\\": \\\"30Mi\\\"\\n\",\"serverFiles\":{\"alerting_rules.yml\":{},\"alerts\":{},\"prometheus.yml\":{\"rule_files\":[\"/etc/config/recording_rules.yml\",\"/etc/config/alerting_rules.yml\",\"/etc/config/rules\",\"/etc/config/alerts\"],\"scrape_configs\":[{\"job_name\":\"prometheus\",\"static_configs\":[{\"targets\":[\"localhost:9090\"]}]},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-apiservers\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"default;kubernetes;https\",\"source_labels\":[\"__meta_kubernetes_namespace\",\"__meta_kubernetes_service_name\",\"__meta_kubernetes_endpoint_port_name\"]}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes-cadvisor\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics/cadvisor\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"honor_labels\":true,\"job_name\":\"kubernetes-service-endpoints\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(.+?)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-service-endpoints-slow\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(.+?)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"},{\"honor_labels\":true,\"job_name\":\"prometheus-pushgateway\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"pushgateway\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-services\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"metrics_path\":\"/probe\",\"params\":{\"module\":[\"http_2xx\"]},\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]},{\"source_labels\":[\"__address__\"],\"target_label\":\"__param_target\"},{\"replacement\":\"blackbox\",\"target_label\":\"__address__\"},{\"source_labels\":[\"__param_target\"],\"target_label\":\"instance\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-pods\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\",\"replacement\":\"[$2]:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\",\"replacement\":\"$2:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-pods-slow\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\",\"replacement\":\"[$2]:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\",\"replacement\":\"$2:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"}]},\"recording_rules.yml\":{},\"rules\":{}},\"serviceAccounts\":{\"server\":{\"annotations\":{},\"create\":true,\"name\":\"\"}}}",
                "version": "45.7.1"
              }
            ],
            "name": "prometheus",
            "namespace": "prometheus",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [
              {
                "name": "podSecurityPolicy.enabled",
                "type": "",
                "value": "true"
              },
              {
                "name": "server.persistentVolume.enabled",
                "type": "",
                "value": "false"
              },
              {
                "name": "server\\.resources",
                "type": "",
                "value": "\"limits\":\n  \"cpu\": \"200m\"\n  \"memory\": \"50Mi\"\n\"requests\":\n  \"cpu\": \"100m\"\n  \"memory\": \"30Mi\"\n"
              }
            ],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 2000,
            "values": [
              "rbac:\n  create: true\n\npodSecurityPolicy:\n  enabled: false\n\nimagePullSecrets: []\n# - name: \"image-pull-secret\"\n\n## Define serviceAccount names for components. Defaults to component's fully qualified name.\n##\nserviceAccounts:\n  server:\n    create: true\n    name: \"\"\n    annotations: {}\n\n## Monitors ConfigMap changes and POSTs to a URL\n## Ref: https://github.com/jimmidyson/configmap-reload\n##\nconfigmapReload:\n  prometheus:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: jimmidyson/configmap-reload\n      tag: v0.8.0\n      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).\n      digest: \"\"\n      pullPolicy: IfNotPresent\n\n    # containerPort: 9533\n\n    ## Additional configmap-reload container arguments\n    ##\n    extraArgs: {}\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n    ## Security context to be added to configmap-reload container\n    containerSecurityContext: {}\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n\nserver:\n  ## Prometheus server container name\n  ##\n  name: server\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: \u003cKey\u003e`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  sidecarContainers: {}\n\n  # sidecarTemplateValues - context to be used in template for sidecarContainers\n  # Example:\n  #   sidecarTemplateValues: *your-custom-globals\n  #   sidecarContainers:\n  #     webserver: |-\n  #       {{ include \"webserver-container-template\" . }}\n  # Template for `webserver-container-template` might looks like this:\n  #   image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\"\n  #   ...\n  #\n  sidecarTemplateValues: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).\n    digest: \"\"\n    pullPolicy: IfNotPresent\n\n  ## prometheus server priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## EnableServiceLinks indicates whether information about services should be injected\n  ## into pod's environment variables, matching the syntax of Docker links.\n  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.\n  ##\n  enableServiceLinks: true\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access prometheus\n  ## Maybe same with Ingress host name\n  baseURL: \"\"\n\n  ## Additional server container environment variables\n  ##\n  ## You specify this manually like you would a raw deployment manifest.\n  ## This means you can bind in environment variables from secrets.\n  ##\n  ## e.g. static environment variable:\n  ##  - name: DEMO_GREETING\n  ##    value: \"Hello from the environment\"\n  ##\n  ## e.g. secret environment variable:\n  ## - name: USERNAME\n  ##   valueFrom:\n  ##     secretKeyRef:\n  ##       name: mysecret\n  ##       key: username\n  env: []\n\n  # List of flags to override default parameters, e.g:\n  # - --enable-feature=agent\n  # - --storage.agent.retention.max-time=30m\n  defaultFlagsOverride: []\n\n  extraFlags:\n    - web.enable-lifecycle\n    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as\n    ## deleting time series. This is disabled by default.\n    # - web.enable-admin-api\n    ##\n    ## storage.tsdb.no-lockfile flag controls BD locking\n    # - storage.tsdb.no-lockfile\n    ##\n    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)\n    # - storage.tsdb.wal-compression\n\n  ## Path to a configuration file on prometheus server container FS\n  configPath: /etc/config/prometheus.yml\n\n  ### The data directory used by prometheus to set --storage.tsdb.path\n  ### When empty server.persistentVolume.mountPath is used instead\n  storagePath: \"\"\n\n  global:\n    ## How frequently to scrape targets by default\n    ##\n    scrape_interval: 1m\n    ## How long until a scrape request times out\n    ##\n    scrape_timeout: 10s\n    ## How frequently to evaluate rules\n    ##\n    evaluation_interval: 1m\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n  ##\n  remoteWrite: []\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read\n  ##\n  remoteRead: []\n\n  ## Custom HTTP headers for Liveness/Readiness/Startup Probe\n  ##\n  ## Useful for providing HTTP Basic Auth to healthchecks\n  probeHeaders: []\n    # - name: \"Authorization\"\n    #   value: \"Bearer ABCDEabcde12345\"\n\n  ## Additional Prometheus server container arguments\n  ##\n  extraArgs: {}\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional Prometheus server Volume mounts\n  ##\n  extraVolumeMounts: []\n\n  ## Additional Prometheus server Volumes\n  ##\n  extraVolumes: []\n\n  ## Additional Prometheus server hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: certs-dir\n    #   mountPath: /etc/kubernetes/certs\n    #   subPath: \"\"\n    #   hostPath: /etc/kubernetes/certs\n    #   readOnly: true\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   subPath: \"\"\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Additional Prometheus server Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: prom-secret-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/server-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ## Extra labels for Prometheus server ConfigMap (ConfigMap that holds serverFiles)\n  extraConfigmapLabels: {}\n\n  ingress:\n    ## If true, Prometheus server Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## Prometheus server Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## Prometheus server Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## Prometheus server Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - prometheus.domain.com\n    #   - domain.com/prometheus\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## Prometheus server Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-server-tls\n    #     hosts:\n    #       - prometheus.domain.com\n\n  ## Server Deployment Strategy type\n  strategy:\n    type: Recreate\n\n  ## hostAliases allows adding entries to /etc/hosts inside the containers\n  hostAliases: []\n  #   - ip: \"127.0.0.1\"\n  #     hostnames:\n  #       - \"example.com\"\n\n  ## Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for Prometheus server pod assignment\n  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, Prometheus server will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## Prometheus server data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## Prometheus server data Persistent Volume labels\n    ##\n    labels: {}\n\n    ## Prometheus server data Persistent Volume annotations\n    ##\n    annotations: {}\n\n    ## Prometheus server data Persistent Volume existing claim name\n    ## Requires server.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## Prometheus server data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## Prometheus server data Persistent Volume size\n    ##\n    size: 8Gi\n\n    ## Prometheus server data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## Prometheus server data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of Prometheus server data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n    ## Persistent Volume Name\n    ## Useful if Persistent Volumes have been provisioned in advance and you want to use a specific one\n    ##\n    # volumeName: \"\"\n\n  emptyDir:\n    ## Prometheus server emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to Prometheus server pods\n  ##\n  podAnnotations: {}\n    # iam.amazonaws.com/role: prometheus\n\n  ## Labels to be added to Prometheus server pods\n  ##\n  podLabels: {}\n\n  ## Prometheus AlertManager configuration\n  ##\n  alertmanagers: []\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n      servicePort: 80\n      ## Enable gRPC port on service to allow auto discovery with thanos-querier\n      gRPC:\n        enabled: false\n        servicePort: 10901\n        # nodePort: 10901\n\n  ## Prometheus server readiness and liveness probe initial delay and timeout\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  ##\n  tcpSocketProbeEnabled: false\n  probeScheme: HTTP\n  readinessProbeInitialDelay: 30\n  readinessProbePeriodSeconds: 5\n  readinessProbeTimeout: 4\n  readinessProbeFailureThreshold: 3\n  readinessProbeSuccessThreshold: 1\n  livenessProbeInitialDelay: 30\n  livenessProbePeriodSeconds: 15\n  livenessProbeTimeout: 10\n  livenessProbeFailureThreshold: 3\n  livenessProbeSuccessThreshold: 1\n  startupProbe:\n    enabled: false\n    periodSeconds: 5\n    failureThreshold: 30\n    timeoutSeconds: 10\n\n  ## Prometheus server resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 500m\n    #   memory: 512Mi\n    # requests:\n    #   cpu: 500m\n    #   memory: 512Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  # When hostNetwork is enabled, this will set to ClusterFirstWithHostNet automatically\n  dnsPolicy: ClusterFirst\n\n  # Use hostPort\n  # hostPort: 9090\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-server'\n\n  # Custom DNS configuration to be added to prometheus server pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to server pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  ## Security context to be added to server container\n  ##\n  containerSecurityContext: {}\n\n  service:\n    ## If false, no Service will be created for the Prometheus server\n    ##\n    enabled: true\n\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    sessionAffinity: None\n    type: ClusterIP\n\n    ## Enable gRPC port on service to allow auto discovery with thanos-querier\n    gRPC:\n      enabled: false\n      servicePort: 10901\n      # nodePort: 10901\n\n    ## If using a statefulSet (statefulSet.enabled=true), configure the\n    ## service to connect to a specific replica to have a consistent view\n    ## of the data.\n    statefulsetReplica:\n      enabled: false\n      replica: 0\n\n  ## Prometheus server pod termination grace period\n  ##\n  terminationGracePeriodSeconds: 300\n\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"15d\"\n\n## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation)\nruleFiles: {}\n\n## Prometheus server ConfigMap entries\n##\nserverFiles:\n  ## Alerts configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n  alerting_rules.yml: {}\n  # groups:\n  #   - name: Instances\n  #     rules:\n  #       - alert: InstanceDown\n  #         expr: up == 0\n  #         for: 5m\n  #         labels:\n  #           severity: page\n  #         annotations:\n  #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'\n  #           summary: 'Instance {{ $labels.instance }} down'\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml\n  alerts: {}\n\n  ## Records configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\n  recording_rules.yml: {}\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml\n  rules: {}\n\n  prometheus.yml:\n    rule_files:\n      - /etc/config/recording_rules.yml\n      - /etc/config/alerting_rules.yml\n    ## Below two files are DEPRECATED will be removed from this default values file\n      - /etc/config/rules\n      - /etc/config/alerts\n\n    scrape_configs:\n      - job_name: prometheus\n        static_configs:\n          - targets:\n            - localhost:9090\n\n      # A scrape configuration for running Prometheus on a Kubernetes cluster.\n      # This uses separate scrape configs for cluster components (i.e. API server, node)\n      # and services to allow each to use different authentication configs.\n      #\n      # Kubernetes labels will be added as Prometheus labels on metrics via the\n      # `labelmap` relabeling action.\n\n      # Scrape config for API servers.\n      #\n      # Kubernetes exposes API servers as endpoints to the default/kubernetes\n      # service so this uses `endpoints` role and uses relabelling to only keep\n      # the endpoints associated with the default/kubernetes service using the\n      # default named port `https`. This works for single API server deployments as\n      # well as HA API server deployments.\n      - job_name: 'kubernetes-apiservers'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        # Keep only the default/kubernetes service endpoints for the https port. This\n        # will add targets for each API server which Kubernetes adds an endpoint to\n        # the default/kubernetes service.\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n            action: keep\n            regex: default;kubernetes;https\n\n      - job_name: 'kubernetes-nodes'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics\n\n\n      - job_name: 'kubernetes-nodes-cadvisor'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        # This configuration will work only on kubelet 1.7.3+\n        # As the scrape endpoints for cAdvisor have changed\n        # if you are using older version you need to change the replacement to\n        # replacement: /api/v1/nodes/$1:4194/proxy/metrics\n        # more info here https://github.com/coreos/prometheus-operator/issues/633\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n\n        # Metric relabel configs to apply to samples before ingestion.\n        # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)\n        # metric_relabel_configs:\n        # - action: labeldrop\n        #   regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n\n      # Scrape config for service endpoints.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape services that have a value of\n      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: (.+?)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Scrape config for slow service endpoints; same as above, but with a larger\n      # timeout and a larger interval\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints-slow'\n        honor_labels: true\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: (.+?)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      - job_name: 'prometheus-pushgateway'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: pushgateway\n\n      # Example scrape config for probing services via the Blackbox Exporter.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/probe`: Only probe services that have a value of `true`\n      - job_name: 'kubernetes-services'\n        honor_labels: true\n\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: true\n          - source_labels: [__address__]\n            target_label: __param_target\n          - target_label: __address__\n            replacement: blackbox\n          - source_labels: [__param_target]\n            target_label: instance\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            target_label: service\n\n      # Example scrape config for pods\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,\n      # except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\n            replacement: '[$2]:$1'\n            target_label: __address__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);((([0-9]+?)(\\.|$)){4})\n            replacement: $2:$1\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n      # Example Scrape config for pods which should be scraped slower. An useful example\n      # would be stackriver-exporter which queries an API on every scrape of the pod\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods-slow'\n        honor_labels: true\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\n            replacement: '[$2]:$1'\n            target_label: __address__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);((([0-9]+?)(\\.|$)){4})\n            replacement: $2:$1\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n\n# adds additional scrape configs to prometheus.yml\n# must be a string so you have to add a | after extraScrapeConfigs:\n# example adds prometheus-blackbox-exporter scrape config\nextraScrapeConfigs: \"\"\n  # - job_name: 'prometheus-blackbox-exporter'\n  #   metrics_path: /probe\n  #   params:\n  #     module: [http_2xx]\n  #   static_configs:\n  #     - targets:\n  #       - https://example.com\n  #   relabel_configs:\n  #     - source_labels: [__address__]\n  #       target_label: __param_target\n  #     - source_labels: [__param_target]\n  #       target_label: instance\n  #     - target_label: __address__\n  #       replacement: prometheus-blackbox-exporter:9115\n\n# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager\n# useful in H/A prometheus with different external labels but the same alerts\nalertRelabelConfigs: {}\n  # alert_relabel_configs:\n  # - source_labels: [dc]\n  #   regex: (.+)\\d+\n  #   target_label: dc\n\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n\n# Force namespace of namespaced resources\nforceNamespace: \"\"\n\n# Extra manifests to deploy as an array\nextraManifests: []\n  # - apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n  #     extra-data: \"value\"\n\n# Configuration of subcharts defined in Chart.yaml\n\n## alertmanager sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/alertmanager\n##\nalertmanager:\n  ## If false, alertmanager will not be installed\n  ##\n  enabled: true\n\n  persistence:\n    size: 2Gi\n\n  podSecurityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n## kube-state-metrics sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics\n##\nkube-state-metrics:\n  ## If false, kube-state-metrics sub-chart will not be installed\n  ##\n  enabled: true\n\n## promtheus-node-exporter sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter\n##\nprometheus-node-exporter:\n  ## If false, node-exporter will not be installed\n  ##\n  enabled: true\n\n  rbac:\n    pspEnabled: false\n\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n\n## pprometheus-pushgateway sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-pushgateway\n##\nprometheus-pushgateway:\n  ## If false, pushgateway will not be installed\n  ##\n  enabled: true\n\n  # Optional service annotations\n  serviceAnnotations:\n    prometheus.io/probe: pushgateway\n"
            ],
            "verify": false,
            "version": "45.7.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.aws_eks_cluster.demo",
            "data.aws_eks_node_group.eks-node-group",
            "kubernetes_namespace.kube-namespace",
            "time_sleep.wait_for_kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "kubernetes_namespace",
      "name": "kube-namespace",
      "provider": "provider[\"registry.terraform.io/hashicorp/kubernetes\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "id": "prometheus",
            "metadata": [
              {
                "annotations": null,
                "generate_name": "",
                "generation": 0,
                "labels": null,
                "name": "prometheus",
                "resource_version": "332660",
                "uid": "92aaf983-497a-45c7-9613-c67b20556e75"
              }
            ],
            "timeouts": null
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiZGVsZXRlIjozMDAwMDAwMDAwMDB9fQ==",
          "dependencies": [
            "data.aws_eks_cluster.demo",
            "data.aws_eks_node_group.eks-node-group",
            "time_sleep.wait_for_kubernetes"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "time_sleep",
      "name": "wait_for_kubernetes",
      "provider": "provider[\"registry.terraform.io/hashicorp/time\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "create_duration": "20s",
            "destroy_duration": null,
            "id": "2023-03-18T21:57:29Z",
            "triggers": null
          },
          "sensitive_attributes": [],
          "dependencies": [
            "data.aws_eks_cluster.demo"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
